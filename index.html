<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Harsh Tomar - AI/ML Developer</title>
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&display=swap" rel="stylesheet">
</head>
<body>
    <div class="container">
        <header>
            <div class="profile">
                <img src="https://avatars.githubusercontent.com/u/123729493?v=4" alt="Harsh Tomar" class="avatar">
                <h1>Harsh Tomar</h1>
                <p class="tagline">Building AI systems that see, understand, and learn</p>
                <p class="bio">AI/ML Engineer specializing in Computer Vision, Deep Learning & Generative AI. 8+ months production experience building intelligent systems. Active open-source contributor with 41+ repositories and CNCF community member.</p>
                <div class="social-links">
                    <a href="https://github.com/HarshTomar1234" target="_blank">GitHub</a>
                    <a href="https://www.linkedin.com/in/harsh-tomar-a96a38256/" target="_blank">LinkedIn</a>
                    <a href="https://x.com/kernel_crush" target="_blank">X</a>
                    <a href="mailto:tomarharsh28303@gmail.com">Email</a>
                </div>
            </div>
        </header>

        <main>
            <section class="featured-projects">
                <h2>Featured Projects</h2>
                <p class="section-desc">Production-ready AI systems with comprehensive technical implementations</p>
                
                <!-- Tennis Vision -->
                <div class="project-detailed">
                    <div class="project-header">
                        <h3><a href="https://github.com/HarshTomar1234/Tennis-Vision" target="_blank">Tennis Vision</a></h3>
                        <div class="project-meta">
                            <span class="stars">⭐ 23</span>
                            <span class="year">2025</span>
                        </div>
                    </div>
                    
                    <p class="project-summary">Advanced computer vision system for comprehensive tennis match analysis achieving 95% player detection accuracy and 88% ball tracking precision. Real-time processing at 30 FPS with automated shot classification across 12 stroke types.</p>
                    
                    <div class="project-images-grid">
                        <img src="https://raw.githubusercontent.com/HarshTomar1234/Tennis-Vision/main/frame_images/tennis_analysis_middle_frame107.png" alt="Tennis Vision - Mid Match Analysis">
                        <img src="https://raw.githubusercontent.com/HarshTomar1234/Tennis-Vision/main/frame_images/tennis_analysis_quarter_frame53.png" alt="Tennis Vision - Quarter Match Analysis">
                        <img src="https://raw.githubusercontent.com/HarshTomar1234/Tennis-Vision/main/frame_images/tennis_analysis_sixty_percent_frame128.png" alt="Tennis Vision - 60% Match Analysis">
                    </div>
                    
                    <div class="tech-details">
                        <h4>Technical Implementation</h4>
                        <ul>
                            <li><strong>Player Detection:</strong> YOLOv8x achieving 92.8% mAP@0.5 with confidence threshold 0.7 and size filtering (20x50px minimum)</li>
                            <li><strong>Ball Tracking:</strong> Custom YOLOv8 model trained on 578 images achieving 87.3% mAP@0.5 with polynomial interpolation for smooth trajectories</li>
                            <li><strong>Court Detection:</strong> ResNet-50 based keypoint detection for 14 landmarks with 91.5% accuracy within 5-pixel tolerance</li>
                            <li><strong>Shot Classification:</strong> Rule-based classifier with 89.4% accuracy analyzing player position, ball trajectory, and temporal context</li>
                            <li><strong>Performance:</strong> 6.67 FPS processing speed with 94% memory reduction through ROI optimization</li>
                        </ul>
                        
                        <h4>Key Features</h4>
                        <ul>
                            <li>Real-time player tracking with position heatmaps and movement analysis</li>
                            <li>Ball trajectory visualization with shot moment identification</li>
                            <li>Automated classification: Serve (95.2%), Forehand (87.8%), Backhand (86.1%), Volley (91.3%), Smash (93.7%)</li>
                            <li>Mini-court visualization providing bird's-eye view of match dynamics</li>
                            <li>Comprehensive statistics dashboard with player speeds and shot analytics</li>
                        </ul>
                        
                        <div class="metrics-grid">
                            <div class="metric">
                                <span class="metric-value">95%</span>
                                <span class="metric-label">Player Detection</span>
                            </div>
                            <div class="metric">
                                <span class="metric-value">88%</span>
                                <span class="metric-label">Ball Tracking</span>
                            </div>
                            <div class="metric">
                                <span class="metric-value">30 FPS</span>
                                <span class="metric-label">Processing Speed</span>
                            </div>
                            <div class="metric">
                                <span class="metric-value">12</span>
                                <span class="metric-label">Stroke Types</span>
                            </div>
                        </div>
                    </div>
                    
                    <div class="tech-stack">PyTorch • OpenCV • YOLOv8 • Supervision • ResNet-50</div>
                </div>

                <!-- QuantaAI -->
                <div class="project-detailed">
                    <div class="project-header">
                        <h3><a href="https://github.com/HarshTomar1234/QuantaAI" target="_blank">QuantaAI</a></h3>
                        <div class="project-meta">
                            <span class="year">2025</span>
                        </div>
                    </div>
                    
                    <p class="project-summary">Intelligent conversational AI with real-time web search capabilities powered by LangGraph for agentic decision-making and GPT-4 for contextual responses. Features transparent multi-stage search pipeline with real-time visualization.</p>
                    
                    <div class="project-images-grid">
                        <img src="https://raw.githubusercontent.com/HarshTomar1234/QuantaAI/main/public/app-interface.png" alt="QuantaAI Chat Interface">
                        <img src="https://raw.githubusercontent.com/HarshTomar1234/QuantaAI/main/public/langgraph-architecture.png" alt="LangGraph Architecture">
                        <img src="https://raw.githubusercontent.com/HarshTomar1234/QuantaAI/main/public/search-stages.png" alt="Search Pipeline Stages">
                    </div>
                    
                    <div class="tech-details">
                        <h4>Architecture</h4>
                        <ul>
                            <li><strong>LangGraph State Machine:</strong> Orchestrates multi-step reasoning with conditional routing between search, read, and write stages</li>
                            <li><strong>Search Agent:</strong> Tavily API integration for real-time web search with relevance scoring and source attribution</li>
                            <li><strong>GPT-4 Integration:</strong> Context-aware response generation with streaming support for real-time user feedback</li>
                            <li><strong>Next.js Frontend:</strong> Server-side rendering with TypeScript for type-safe development and optimal performance</li>
                        </ul>
                        
                        <h4>Key Features</h4>
                        <ul>
                            <li>Real-time search visualization showing AI's decision-making process</li>
                            <li>Multi-source information aggregation with automatic fact verification</li>
                            <li>Streaming responses for improved user experience and perceived performance</li>
                            <li>Source attribution with clickable references for transparency</li>
                        </ul>
                    </div>
                    
                    <div class="tech-stack">LangGraph • GPT-4 • Next.js • TypeScript • Tavily API • Vercel</div>
                </div>

                <!-- MoleCuQuest -->
                <div class="project-detailed">
                    <div class="project-header">
                        <h3><a href="https://github.com/HarshTomar1234/MoleCuQuest" target="_blank">MoleCuQuest</a></h3>
                        <div class="project-meta">
                            <span class="year">2025</span>
                        </div>
                    </div>
                    
                    <p class="project-summary">Advanced molecular research platform combining AI-driven molecule generation with comprehensive drug discovery tools. Features NVIDIA MolMIM integration, 3D visualization, and collaborative research environment.</p>
                    
                    <div class="project-images-grid">
                        <img src="https://raw.githubusercontent.com/HarshTomar1234/MoleCuQuest/main/public/dashboard.png" alt="MoleCuQuest Dashboard">
                        <img src="https://raw.githubusercontent.com/HarshTomar1234/MoleCuQuest/main/public/molecule-generator.png" alt="AI Molecule Generator">
                        <img src="https://raw.githubusercontent.com/HarshTomar1234/MoleCuQuest/main/public/molecules.png" alt="3D Molecular Structures">
                        <img src="https://raw.githubusercontent.com/HarshTomar1234/MoleCuQuest/main/public/molecule-bank.png" alt="Molecule Database">
                    </div>
                    
                    <div class="tech-details">
                        <h4>Core Technologies</h4>
                        <ul>
                            <li><strong>NVIDIA MolMIM:</strong> State-of-the-art molecular generation using masked language modeling on SMILES representations</li>
                            <li><strong>RDKit Integration:</strong> 3D molecular visualization, property calculation, and structure validation</li>
                            <li><strong>CMA-ES Optimization:</strong> Covariance Matrix Adaptation Evolution Strategy for molecular property optimization</li>
                            <li><strong>QED Scoring:</strong> Quantitative Estimate of Drug-likeness for pharmaceutical viability assessment</li>
                            <li><strong>PubChem API:</strong> Access to 100M+ chemical compounds for reference and validation</li>
                        </ul>
                        
                        <h4>Research Capabilities</h4>
                        <ul>
                            <li>AI-powered de novo molecule generation with property constraints</li>
                            <li>Interactive 3D molecular structure visualization and manipulation</li>
                            <li>Comprehensive property prediction (LogP, TPSA, molecular weight, etc.)</li>
                            <li>Collaborative workspace for team-based drug discovery projects</li>
                            <li>Integration with chemical databases for literature mining</li>
                        </ul>
                    </div>
                    
                    <div class="tech-stack">NVIDIA MolMIM • RDKit • CMA-ES • Next.js • PubChem API • Three.js</div>
                </div>

                <!-- Field Fusion -->
                <div class="project-detailed">
                    <div class="project-header">
                        <h3><a href="https://github.com/HarshTomar1234/Field-Fusion" target="_blank">Field Fusion</a></h3>
                        <div class="project-meta">
                            <span class="year">2024</span>
                        </div>
                    </div>
                    
                    <p class="project-summary">Comprehensive football match analysis system using computer vision for player tracking, team classification, and tactical insights. Real-time processing with advanced metrics calculation.</p>
                    
                    <div class="project-image">
                        <img src="https://raw.githubusercontent.com/HarshTomar1234/Field-Fusion/main/output_videos/screenshot.png" alt="Field Fusion Match Analysis">
                    </div>
                    
                    <div class="tech-details">
                        <h4>Technical Features</h4>
                        <ul>
                            <li><strong>Player Detection:</strong> YOLOv8 for multi-player tracking with team classification using K-means clustering on jersey colors</li>
                            <li><strong>Ball Tracking:</strong> Custom trained model with trajectory prediction and possession analysis</li>
                            <li><strong>Camera Movement:</strong> Optical flow estimation for stabilized tracking across camera pans and zooms</li>
                            <li><strong>Speed Analysis:</strong> Real-time player speed calculation with distance covered metrics</li>
                        </ul>
                    </div>
                    
                    <div class="tech-stack">YOLOv8 • OpenCV • K-means • Optical Flow • ByteTrack</div>
                </div>

                <!-- RAGify -->
                <div class="project-detailed">
                    <div class="project-header">
                        <h3><a href="https://github.com/HarshTomar1234/RAGify" target="_blank">RAGify</a></h3>
                        <div class="project-meta">
                            <span class="stars">⭐ 1</span>
                            <span class="year">2024</span>
                        </div>
                    </div>
                    
                    <p class="project-summary">Comprehensive Retrieval-Augmented Generation (RAG) implementation from scratch. Educational repository demonstrating core RAG concepts, vector databases, and semantic search pipelines.</p>
                    
                    <div class="project-image">
                        <img src="https://raw.githubusercontent.com/HarshTomar1234/RAGify/main/images/rag-workflow.png" alt="RAG Workflow Architecture">
                    </div>
                    
                    <div class="tech-details">
                        <h4>Implementation Details</h4>
                        <ul>
                            <li><strong>Document Processing:</strong> Text chunking strategies with overlap for context preservation</li>
                            <li><strong>Embedding Generation:</strong> Sentence transformers for semantic vector representations</li>
                            <li><strong>Vector Storage:</strong> FAISS and Chroma DB implementations for efficient similarity search</li>
                            <li><strong>Retrieval Pipeline:</strong> Hybrid search combining dense and sparse retrieval methods</li>
                            <li><strong>Generation:</strong> LLM integration with context injection and prompt engineering</li>
                        </ul>
                    </div>
                    
                    <div class="tech-stack">LangChain • FAISS • Chroma • Sentence Transformers • OpenAI</div>
                </div>

                <!-- TinderMuse -->
                <div class="project-detailed">
                    <div class="project-header">
                        <h3><a href="https://github.com/HarshTomar1234/tindermuse" target="_blank">TinderMuse</a></h3>
                        <div class="project-meta">
                            <span class="stars">⭐ 1</span>
                            <span class="year">2024</span>
                        </div>
                    </div>
                    
                    <p class="project-summary">AI-powered Tinder profile image generator using advanced image enhancement and style transfer techniques. Transforms ordinary photos into attention-grabbing profile pictures.</p>
                    
                    <div class="project-images-grid">
                        <img src="https://raw.githubusercontent.com/HarshTomar1234/tindermuse/main/public/interface.png" alt="TinderMuse Interface">
                        <img src="https://raw.githubusercontent.com/HarshTomar1234/tindermuse/main/public/profile-example.png" alt="Profile Enhancement Example">
                        <img src="https://raw.githubusercontent.com/HarshTomar1234/tindermuse/main/public/tindermuse-app.png" alt="TinderMuse Application">
                    </div>
                    
                    <div class="tech-details">
                        <h4>AI Capabilities</h4>
                        <ul>
                            <li><strong>Image Enhancement:</strong> AI-driven color correction, lighting optimization, and detail enhancement</li>
                            <li><strong>Background Removal:</strong> Automatic subject segmentation with edge refinement</li>
                            <li><strong>Style Transfer:</strong> Multiple artistic styles and filters for personalized aesthetics</li>
                            <li><strong>Face Detection:</strong> Automatic cropping and framing for optimal composition</li>
                        </ul>
                    </div>
                    
                    <div class="tech-stack">Stable Diffusion • OpenCV • Next.js • Replicate API</div>
                </div>
            </section>

            <section class="research-implementation">
                <h2>Research Paper Implementation</h2>
                <p class="section-desc">Deep dives into cutting-edge AI research with from-scratch implementations</p>
                
                <!-- VLMverse -->
                <div class="project-detailed">
                    <div class="project-header">
                        <h3><a href="https://github.com/HarshTomar1234/VLMverse" target="_blank">VLMverse: Vision-Language Models</a></h3>
                        <div class="project-meta">
                            <span class="year">2025</span>
                        </div>
                    </div>
                    
                    <p class="project-summary">Complete PyTorch implementation of PaLiGemma 3B vision-language model from scratch. Combines Google's Gemma language model with SigLIP vision encoder for multimodal understanding. Detailed architectural breakdown with educational code demonstrating how VLMs process images and text to generate contextually relevant responses.</p>
                    
                    <div class="project-images-grid">
                        <img src="https://raw.githubusercontent.com/HarshTomar1234/VLMverse/main/images/VLMs%20architecture.png" alt="Vision-Language Model Architecture">
                        <img src="https://raw.githubusercontent.com/HarshTomar1234/VLMverse/main/images/PaliGemma%203B%20VLM%20implementation%20.png" alt="PaLiGemma Implementation">
                        <img src="https://raw.githubusercontent.com/HarshTomar1234/VLMverse/main/images/SigLip%20ViT.png" alt="SigLIP Vision Transformer">
                        <img src="https://raw.githubusercontent.com/HarshTomar1234/VLMverse/main/images/RoPE%20embeddings.png" alt="Rotary Position Embeddings">
                    </div>
                    
                    <div class="tech-details">
                        <h4>Architecture Components</h4>
                        <ul>
                            <li><strong>SigLIP Vision Encoder:</strong> Improved CLIP variant using sigmoid loss instead of contrastive loss. Processes 224×224 images into 196 patch embeddings (16×16 patches). Features Vision Transformer with learned positional embeddings, multi-head self-attention, and feed-forward networks with GeLU activations.</li>
                            <li><strong>Gemma Language Model:</strong> Decoder-only transformer with 2B parameters. Uses RMSNorm for training stability, RoPE (Rotary Position Encoding) for positional information, and grouped-query attention for computational efficiency. Implements flash attention and KV-caching for fast inference.</li>
                            <li><strong>Multimodal Fusion:</strong> Linear projection layer maps vision embeddings to language model's embedding space. Image tokens prepended to text tokens, enabling cross-modal attention. Supports visual question answering, image captioning, and visual reasoning tasks.</li>
                            <li><strong>RoPE Implementation:</strong> Rotary position encoding applies 2D rotation matrices to query/key vectors. Rotation angle θⱼ = 10000^(-2j/d) encodes relative positions. Enables length extrapolation beyond training sequence lengths.</li>
                        </ul>
                        
                        <h4>Key Innovations</h4>
                        <ul>
                            <li>SigLIP's sigmoid loss enables better hard negative handling and more stable training compared to CLIP's contrastive loss</li>
                            <li>Grouped-query attention reduces KV cache size by 8x while maintaining 95% of full attention performance</li>
                            <li>RoPE encoding provides better length generalization than absolute or learned positional embeddings</li>
                            <li>Pre-trained weights from Hugging Face enable immediate inference without training</li>
                        </ul>
                        
                        <h4>Implementation Highlights</h4>
                        <ul>
                            <li>Complete modeling code in modeling_siglip.py (vision) and modeling_gemma.py (language)</li>
                            <li>Inference pipeline with image preprocessing, tokenization, and autoregressive generation</li>
                            <li>KV-cache implementation for 3x faster generation speed</li>
                            <li>Detailed documentation explaining every architectural choice</li>
                        </ul>
                    </div>
                    
                    <div class="tech-stack">PyTorch • Transformers • SigLIP • Gemma • RoPE • Flash Attention</div>
                </div>

                <!-- PyTorch LoRA-QLoRA -->
                <div class="project-detailed">
                    <div class="project-header">
                        <h3><a href="https://github.com/HarshTomar1234/PyTorch-LoRA-QLoRA" target="_blank">PyTorch LoRA & QLoRA</a></h3>
                        <div class="project-meta">
                            <span class="year">2025</span>
                        </div>
                    </div>
                    
                    <p class="project-summary">Pure PyTorch implementations of LoRA (Low-Rank Adaptation) and QLoRA (Quantized LoRA) for memory-efficient fine-tuning of large language models. Reduces memory requirements by 65-85% while maintaining performance comparable to full fine-tuning. Includes practical examples for text and image classification.</p>
                    
                    <div class="project-images-grid">
                        <img src="https://raw.githubusercontent.com/HarshTomar1234/PyTorch-LoRA-QLoRA/main/images/lora_architecture.svg" alt="LoRA Architecture">
                        <img src="https://raw.githubusercontent.com/HarshTomar1234/PyTorch-LoRA-QLoRA/main/images/qlora_architecture.svg" alt="QLoRA Architecture">
                        <img src="https://raw.githubusercontent.com/HarshTomar1234/PyTorch-LoRA-QLoRA/main/images/memory_comparison.svg" alt="Memory Efficiency Comparison">
                    </div>
                    
                    <div class="tech-details">
                        <h4>LoRA Implementation</h4>
                        <ul>
                            <li><strong>Low-Rank Decomposition:</strong> Freezes pre-trained weights W and adds trainable matrices A ∈ ℝᵈˣʳ and B ∈ ℝʳˣᵏ where r ≪ min(d,k). Forward pass: y = xW + x(AB) = x(W + AB). Typical rank r = 8, 16, or 32 reduces trainable parameters by 99%.</li>
                            <li><strong>Scaling Factor:</strong> Uses α/r scaling to control update magnitude. Standard LoRA: W' = W + α·AB/r. Rank-stabilized variant: W' = W + α·AB/√r for better numerical stability.</li>
                            <li><strong>Layer Selection:</strong> Typically applied to query/value projection matrices in attention layers. Can also target feed-forward layers for additional capacity.</li>
                            <li><strong>Initialization:</strong> Matrix A initialized with Kaiming uniform, matrix B initialized to zeros. Ensures LoRA starts as identity transformation.</li>
                        </ul>
                        
                        <h4>QLoRA Innovations</h4>
                        <ul>
                            <li><strong>4-bit NF4 Quantization:</strong> Normal Float 4-bit quantization optimized for weight distributions in neural networks. Quantizes frozen weights to 4-bit while keeping LoRA adapters in BF16 precision.</li>
                            <li><strong>Double Quantization:</strong> Quantizes the quantization constants themselves, saving additional 0.4 bits per parameter. Reduces memory by 3% with negligible accuracy impact.</li>
                            <li><strong>Paged Optimizers:</strong> Offloads optimizer states to CPU RAM when GPU memory is full. Enables fine-tuning of 65B models on 48GB GPUs.</li>
                            <li><strong>Gradient Checkpointing:</strong> Trades computation for memory by recomputing activations during backward pass. Combined with QLoRA enables 70B model fine-tuning on consumer hardware.</li>
                        </ul>
                        
                        <h4>Practical Applications</h4>
                        <ul>
                            <li>Text classification with BERT: 99.2% accuracy with only 0.8% trainable parameters</li>
                            <li>Vision Transformer fine-tuning: 94.7% on CIFAR-10 with 1.2M trainable params vs 86M full fine-tuning</li>
                            <li>LLaMA-7B instruction tuning: Matches full fine-tuning quality with 65% less memory</li>
                            <li>Multi-task learning: Train separate LoRA adapters for different tasks, switch at inference time</li>
                        </ul>
                        
                        <h4>Code Structure</h4>
                        <ul>
                            <li>lora.py: Core LoRA layer implementation with configurable rank and scaling</li>
                            <li>qlora.py: QLoRA with 4-bit quantization and double quantization</li>
                            <li>train_text_classifier.py: BERT fine-tuning example on sentiment analysis</li>
                            <li>train_vit_image_classifier.py: Vision Transformer fine-tuning on CIFAR-10</li>
                        </ul>
                    </div>
                    
                    <div class="tech-stack">PyTorch • LoRA • QLoRA • 4-bit Quantization • PEFT • BitsAndBytes</div>
                </div>

                <!-- Paper Replications -->
                <div class="project-detailed">
                    <div class="project-header">
                        <h3><a href="https://github.com/HarshTomar1234/Paper-Replications" target="_blank">Paper Replications</a></h3>
                        <div class="project-meta">
                            <span class="year">2024-2025</span>
                        </div>
                    </div>
                    
                    <p class="project-summary">Comprehensive repository of 39+ classic and state-of-the-art AI/ML paper implementations in PyTorch. From foundational architectures like BERT and GPT to cutting-edge models like DeepSeek-V3, Llama 4, and Differential Transformers. Each implementation includes detailed documentation, training scripts, and architectural explanations.</p>
                    
                    <div class="tech-details">
                        <h4>Vision-Language Models</h4>
                        <ul>
                            <li><strong>PaLiGemma:</strong> Google's 3B parameter VLM combining SigLIP vision encoder with Gemma language model. Implements multimodal fusion, grouped-query attention, and RoPE positional encoding.</li>
                            <li><strong>LLaVA:</strong> Large Language and Vision Assistant with visual instruction tuning. Features vision-language connector, multi-turn conversations, and visual reasoning capabilities.</li>
                            <li><strong>CLIP:</strong> Contrastive Language-Image Pre-training with dual encoder architecture. Implements contrastive loss, temperature scaling, and zero-shot classification.</li>
                            <li><strong>CLAP:</strong> Contrastive Language-Audio Pre-training extending CLIP to audio domain. Audio encoder with mel-spectrogram processing and cross-modal retrieval.</li>
                        </ul>
                        
                        <h4>Large Language Models</h4>
                        <ul>
                            <li><strong>DeepSeek-V3:</strong> Latest 671B parameter MoE model with Multi-head Latent Attention and auxiliary-loss-free load balancing. Implements FP8 mixed precision training.</li>
                            <li><strong>Llama 4:</strong> Meta's latest open-source LLM with grouped-query attention, RMSNorm, and SwiGLU activations. Includes pre-training and instruction tuning code.</li>
                            <li><strong>Gemma 3:</strong> Google's lightweight 2B/7B models with multi-query attention and knowledge distillation from larger models.</li>
                            <li><strong>GPT:</strong> Decoder-only transformer with causal masking, learned positional embeddings, and autoregressive generation.</li>
                            <li><strong>BERT:</strong> Bidirectional encoder with masked language modeling and next sentence prediction objectives.</li>
                        </ul>
                        
                        <h4>Advanced Architectures</h4>
                        <ul>
                            <li><strong>Differential Transformer:</strong> Novel attention mechanism with differential attention scores. Reduces noise amplification and improves long-context modeling.</li>
                            <li><strong>Mixtral MoE:</strong> Mixture of Experts with 8 expert networks and top-2 routing. Implements load balancing loss and expert capacity constraints.</li>
                            <li><strong>Kimi-K2:</strong> Long-context model with 200K token window using sparse attention and memory-efficient transformers.</li>
                        </ul>
                        
                        <h4>Generative Models</h4>
                        <ul>
                            <li><strong>DCGANs:</strong> Deep Convolutional GANs with batch normalization, LeakyReLU, and transposed convolutions for image generation.</li>
                            <li><strong>CGANs:</strong> Conditional GANs with class conditioning for controlled generation.</li>
                            <li><strong>CycleGANs:</strong> Unpaired image-to-image translation with cycle consistency loss.</li>
                            <li><strong>Pix2Pix:</strong> Paired image translation with U-Net generator and PatchGAN discriminator.</li>
                        </ul>
                        
                        <h4>Fine-Tuning Methods</h4>
                        <ul>
                            <li><strong>LoRA:</strong> Low-rank adaptation for parameter-efficient fine-tuning with rank decomposition.</li>
                            <li><strong>DPO:</strong> Direct Preference Optimization for RLHF without reward model training.</li>
                            <li><strong>ORPO:</strong> Odds Ratio Preference Optimization combining SFT and preference learning.</li>
                            <li><strong>PEFT:</strong> Parameter-Efficient Fine-Tuning with adapters, prefix tuning, and prompt tuning.</li>
                        </ul>
                        
                        <h4>Specialized Models</h4>
                        <ul>
                            <li><strong>Moonshine:</strong> Efficient speech recognition with streaming capabilities and low-latency inference.</li>
                            <li><strong>Attention Mechanisms:</strong> Multi-head, multi-query, grouped-query, and flash attention implementations.</li>
                            <li><strong>Encoder-Decoder:</strong> Seq2seq with attention for machine translation and summarization.</li>
                            <li><strong>GRU:</strong> Gated Recurrent Units for sequence modeling with reset and update gates.</li>
                        </ul>
                    </div>
                    
                    <div class="tech-stack">PyTorch • Transformers • Computer Vision • NLP • Generative AI • RLHF</div>
                </div>

                <!-- Reasoning LLMs -->
                <div class="project-detailed">
                    <div class="project-header">
                        <h3><a href="https://github.com/HarshTomar1234/Reasoning-llms-" target="_blank">Reasoning LLMs from Scratch</a></h3>
                        <div class="project-meta">
                            <span class="year">2025</span>
                        </div>
                    </div>
                    
                    <p class="project-summary">Comprehensive exploration of reasoning capabilities in Large Language Models implemented from scratch. Covers inference-time compute scaling, reinforcement learning approaches, chain-of-thought mechanisms, and advanced reasoning techniques for building more capable AI systems.</p>
                    
                    <div class="tech-details">
                        <h4>Inference-Time Compute Scaling</h4>
                        <ul>
                            <li><strong>Test-Time Compute:</strong> Allocates additional computation during inference rather than just training. Implements best-of-N sampling, beam search, and self-consistency decoding for improved reasoning.</li>
                            <li><strong>Adaptive Computation:</strong> Dynamically adjusts compute based on problem difficulty. Easy questions use single forward pass, hard questions trigger multi-step reasoning with verification.</li>
                            <li><strong>Scaling Laws:</strong> Empirical analysis showing reasoning accuracy improves log-linearly with test-time compute. Demonstrates 2-3x accuracy gains on math/coding tasks with 10x more inference compute.</li>
                            <li><strong>Compute-Optimal Strategies:</strong> Compares different inference strategies (sampling, beam search, self-refinement) across compute budgets. Identifies optimal allocation for different task types.</li>
                        </ul>
                        
                        <h4>Reinforcement Learning for Reasoning</h4>
                        <ul>
                            <li><strong>Outcome-Based RL:</strong> Trains models using final answer correctness as reward signal. Implements REINFORCE with baseline and PPO for stable training.</li>
                            <li><strong>Process-Based RL:</strong> Provides step-by-step feedback on reasoning process. Uses process reward models (PRMs) to score intermediate reasoning steps.</li>
                            <li><strong>Self-Play Training:</strong> Model generates reasoning traces, evaluates them, and learns from successful strategies. Implements iterative refinement with rejection sampling.</li>
                            <li><strong>Reward Modeling:</strong> Trains separate reward models to evaluate reasoning quality. Combines outcome rewards (correctness) with process rewards (reasoning quality).</li>
                        </ul>
                        
                        <h4>Chain-of-Thought Mechanisms</h4>
                        <ul>
                            <li><strong>Zero-Shot CoT:</strong> Prompts like "Let's think step by step" elicit reasoning without examples. Implements prompt engineering strategies for different domains.</li>
                            <li><strong>Few-Shot CoT:</strong> Provides exemplar reasoning traces in context. Demonstrates 3-5x accuracy improvements on complex reasoning tasks.</li>
                            <li><strong>Self-Consistency:</strong> Samples multiple reasoning paths and takes majority vote. Implements temperature-based sampling and answer aggregation strategies.</li>
                            <li><strong>Tree-of-Thoughts:</strong> Explores multiple reasoning branches in parallel. Uses breadth-first and depth-first search with pruning based on intermediate evaluations.</li>
                        </ul>
                        
                        <h4>Advanced Reasoning Techniques</h4>
                        <ul>
                            <li><strong>Least-to-Most Prompting:</strong> Decomposes complex problems into simpler subproblems. Solves subproblems sequentially, using previous solutions as context.</li>
                            <li><strong>Maieutic Prompting:</strong> Generates explanations for both correct and incorrect answers. Uses consistency checking to identify and eliminate faulty reasoning.</li>
                            <li><strong>Reflexion:</strong> Self-reflection on failed attempts to improve future reasoning. Maintains episodic memory of mistakes and successful strategies.</li>
                            <li><strong>Verify-and-Edit:</strong> Generates initial solution, verifies correctness, and iteratively refines. Implements verification models and editing strategies.</li>
                        </ul>
                        
                        <h4>Implementation Details</h4>
                        <ul>
                            <li>Jupyter notebooks with step-by-step explanations of each reasoning technique</li>
                            <li>PyTorch implementations of RL algorithms (REINFORCE, PPO) for reasoning</li>
                            <li>Evaluation harnesses for math (GSM8K), coding (HumanEval), and commonsense reasoning</li>
                            <li>Visualization tools for reasoning traces and decision trees</li>
                        </ul>
                    </div>
                    
                    <div class="tech-stack">PyTorch • Reinforcement Learning • Chain-of-Thought • Tree-of-Thoughts • PPO</div>
                </div>
            </section>

            <section class="more-projects">
                <h2>Additional Projects</h2>
                <div class="project-grid">
                    <div class="project-card">
                        <h3><a href="https://github.com/HarshTomar1234/Multiple_Pdf_ChatApp" target="_blank">Multiple PDF Chat App</a></h3>
                        <p>Conversational AI application for querying multiple PDF documents simultaneously using RAG architecture. Features semantic search, context-aware responses, and document source attribution.</p>
                        <div class="tech-stack">LangChain • RAG • PDF Processing • Vector DB</div>
                    </div>
                    
                    <div class="project-card">
                        <h3><a href="https://github.com/HarshTomar1234/Google-Agent-Development-kit-ADK-" target="_blank">Google ADK Experiments</a></h3>
                        <p>Exploration of Google's Agent Development Kit (ADK) for building intelligent agents with tool integration, function calling, and multi-step reasoning capabilities.</p>
                        <div class="tech-stack">Google ADK • Agent Development • Function Calling</div>
                    </div>
                    
                    <div class="project-card">
                        <h3><a href="https://github.com/HarshTomar1234/web-search-agent" target="_blank">Web Search Agent</a></h3>
                        <p>Intelligent web search agent with query understanding, result ranking, and answer synthesis. Implements semantic search and multi-source information aggregation.</p>
                        <div class="tech-stack">LangChain • Web Search • NLP • Information Retrieval</div>
                    </div>
                    
                    <div class="project-card">
                        <h3><a href="https://github.com/HarshTomar1234/LLMops" target="_blank">LLMOps</a></h3>
                        <p>Comprehensive guide to LLM operations covering deployment, monitoring, maintenance, and improvement of LLM applications at scale. Production best practices and tooling.</p>
                        <div class="tech-stack">MLOps • LLM Deployment • Monitoring • Production</div>
                    </div>
                    
                    <div class="project-card">
                        <h3><a href="https://github.com/HarshTomar1234/cursor-agent" target="_blank">Cursor Agent</a></h3>
                        <p>Python-based AI agent replicating Cursor's coding assistant capabilities with function calling, code generation, and intelligent coding assistance using Claude, OpenAI, and Ollama.</p>
                        <div class="tech-stack">Python • AI Agents • Code Generation • Function Calling</div>
                    </div>
                    
                    <div class="project-card">
                        <h3><a href="https://github.com/HarshTomar1234/reasoning-from-scratch" target="_blank">Reasoning from Scratch</a></h3>
                        <p>Implementation of reasoning LLM in PyTorch from scratch, step by step. Explores chain-of-thought, tree-of-thought, and other reasoning mechanisms in language models.</p>
                        <div class="tech-stack">PyTorch • Reasoning • LLM • From Scratch</div>
                    </div>
                    
                    <div class="project-card">
                        <h3><a href="https://github.com/HarshTomar1234/Machine-and-Deep-Learning-NLP" target="_blank">ML/DL/NLP Learning</a></h3>
                        <p>Comprehensive collection of machine learning, deep learning, and NLP implementations covering fundamental algorithms to advanced architectures.</p>
                        <div class="tech-stack">ML • DL • NLP • PyTorch • TensorFlow</div>
                    </div>
                    
                    <div class="project-card">
                        <h3><a href="https://github.com/HarshTomar1234/dog-vs-cat-classifier" target="_blank">Dog vs Cat Classifier</a></h3>
                        <p>CNN-based image classifier for binary classification with data augmentation, transfer learning, and model optimization techniques.</p>
                        <div class="tech-stack">CNN • Transfer Learning • Image Classification</div>
                    </div>
                </div>
            </section>

            <section class="skills">
                <h2>Technical Expertise</h2>
                <div class="skills-grid">
                    <div class="skill-category">
                        <h3>Computer Vision</h3>
                        <div class="skill-tags">
                            <span>YOLOv5-v8</span>
                            <span>OpenCV</span>
                            <span>Object Detection</span>
                            <span>Image Segmentation</span>
                            <span>Supervision</span>
                            <span>Optical Flow</span>
                            <span>ResNet</span>
                            <span>ByteTrack</span>
                        </div>
                    </div>
                    
                    <div class="skill-category">
                        <h3>ML/AI Frameworks</h3>
                        <div class="skill-tags">
                            <span>PyTorch</span>
                            <span>TensorFlow</span>
                            <span>Scikit-learn</span>
                            <span>HuggingFace</span>
                            <span>Keras</span>
                            <span>XGBoost</span>
                            <span>SHAP</span>
                            <span>MLflow</span>
                        </div>
                    </div>
                    
                    <div class="skill-category">
                        <h3>Generative AI & LLM</h3>
                        <div class="skill-tags">
                            <span>LangChain</span>
                            <span>LangGraph</span>
                            <span>LlamaIndex</span>
                            <span>CrewAI</span>
                            <span>RAG</span>
                            <span>Prompt Engineering</span>
                            <span>OpenAI API</span>
                            <span>Google Gemini</span>
                        </div>
                    </div>
                    
                    <div class="skill-category">
                        <h3>Web & Deployment</h3>
                        <div class="skill-tags">
                            <span>Next.js</span>
                            <span>React</span>
                            <span>TypeScript</span>
                            <span>FastAPI</span>
                            <span>Streamlit</span>
                            <span>Docker</span>
                            <span>AWS</span>
                            <span>CI/CD</span>
                        </div>
                    </div>
                </div>
            </section>

            <section class="experience">
                <h2>Professional Experience</h2>
                
                <div class="experience-item">
                    <div class="exp-header">
                        <h3>AI Intern</h3>
                        <span class="company">i3 Digital Health</span>
                        <span class="duration">May 2025 - Present</span>
                    </div>
                    <p>Architecting intelligent research profiling systems aggregating 10,000+ research papers from multiple APIs, reducing manual research time by 85%. Built NLP pipelines using LangChain achieving 92% accuracy in topic classification. Developed RAG-powered search agents improving match relevance by 78%. Deployed scalable AI solutions serving 500+ researchers using FastAPI, Docker, and AWS infrastructure.</p>
                    <div class="achievements">
                        <span class="achievement">85% time reduction</span>
                        <span class="achievement">92% classification accuracy</span>
                        <span class="achievement">120+ users served</span>
                    </div>
                </div>
                
                <div class="experience-item">
                    <div class="exp-header">
                        <h3>Community Contributor</h3>
                        <span class="company">CNCF & Google Developer Groups</span>
                        <span class="duration">Jan 2023 - Present</span>
                    </div>
                    <p>Active member of Cloud Native Computing Foundation participating in 15+ cloud-native technology discussions. Engaged in Google Developer Groups collaborating on machine learning initiatives, presenting at 2 tech talks on AI/ML best practices. Mentored 10+ junior developers through community workshops and open-source contributions.</p>
                    <div class="achievements">
                        <span class="achievement">15+ discussions</span>
                        <span class="achievement">2 tech talks</span>
                        <span class="achievement">10+ mentees</span>
                    </div>
                </div>
            </section>

            <section class="achievements-section">
                <h2>Achievements & Impact</h2>
                <div class="achievements-grid">
                    <div class="achievement-card">
                        <div class="achievement-number">47+</div>
                        <div class="achievement-label">GitHub Repositories</div>
                    </div>
                    <div class="achievement-card">
                        <div class="achievement-number">40+</div>
                        <div class="achievement-label">GitHub Stars</div>
                    </div>
                    <div class="achievement-card">
                        <div class="achievement-number">1+</div>
                        <div class="achievement-label">Years Experience in AI/ML space</div>
                    </div>
                    <div class="achievement-card">
                        <div class="achievement-number">83%</div>
                        <div class="achievement-label">Accuracy Improvements</div>
                    </div>
                </div>
            </section>

            <section class="education">
                <h2>Education</h2>
                <div class="education-item">
                    <h3>Bachelor of Technology in AI & Data Science</h3>
                    <p class="institution">Lakshmi Narain College of Technology, Bhopal</p>
                    <p class="duration">Nov 2022 - May 2026 • CGPA: 7.2/10</p>
                    <p class="coursework">Relevant Coursework: Machine Learning, Computer Vision, Deep Learning, NLP, Data Structures & Algorithms, Reinforcement Learning, Statistical Analysis, Neural Networks</p>
                </div>
            </section>

            <section class="blogs">
                <h2>Technical Writings</h2>
                <div class="blog-grid">
                    <div class="blog-card">
                        <h3><a href="https://www.notion.so/Tennis-Vision-25b4df040c1480d1840ad41d281672f3" target="_blank">Tennis Vision: Deep Dive</a></h3>
                        <p>Comprehensive analysis of building an AI-powered tennis analysis system with computer vision techniques, model training, and performance optimization strategies.</p>
                        <span class="status">In Progress</span>
                    </div>
                    
                    <div class="blog-card">
                        <h3><a href="https://www.notion.so/Core-Concepts-of-Reasoning-in-LLMs-from-Scratch-1de4df040c14804b9b64f034e181aa75" target="_blank">Reasoning in LLMs from Scratch</a></h3>
                        <p>Exploring core concepts of reasoning capabilities in Large Language Models, implementation details, and architectural considerations for building reasoning systems.</p>
                        <span class="status">In Progress</span>
                    </div>
                </div>
            </section>
        </main>

        <footer>
            <p>Built with curiosity and code • <a href="https://github.com/HarshTomar1234/cool" target="_blank">View Source</a></p>
        </footer>
    </div>
</body>
</html>